---
title: "Homework assignment 2"
subtitle: "Data Science 2: Machine Learning Tools - CEU 2021"
author: "Viktória Mészáros"
date: "03/07/2021"
output: html_document
---



```{r, include=F}
library(tidyverse)
library(kableExtra)
library(keras)
use_python("/usr/local/bin/python3")

my_path <- "C://Users/MViki/Documents/CEU/Winter_semester/DS_2/DS2_Assignment2/Data/"

```


# 1. Fashion MNIST data

In this project I am going to build several deep neural net models with the aim to predict image classes. The data I am using for the project contains images about fashion products. It is called the “Fashion MNIST dataset” which can be downloaded directly from Keras. There are 10 different product categories in the data. 

```{r, include=F}
fashion_mnist <- dataset_fashion_mnist()
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y

```

```{r, echo=F, message=F, warning=F}
Number <- c(0,1,2,3,4,5,6,7,8,9)
Name <- c("T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot")

categories <- data.frame(Number, Name)

categories %>% 
  kbl() %>%
  kable_paper(full_width = F, html_font = "Cambria")

```

We have two data sets, a train set that include 60 000 images and the test set that has 10 000 images. I am going to use the train set to build the models. I will also separate a part of the train set (20%) as a validation set to measure the accuracy of my models. I will use the test set to only get the final accuracy of my models. 
The goal of this exercise to have as accurate classifier as possible: we are using accuracy as a measure of predictive power.

```{r, include=F}
dim(x_train)
dim(y_train)

dim(x_test)
dim(y_test)
```



## a) Show some example images from the data.
Let's look at what we have in the data. These are the first 12 images in the train data. I also put their categories as a label below them. (In the models these will be categories, numbers between 0-9, as showed above). All images are 28x28 pixel grayscale images. They are quite blurry due to this low pixel number, but they are quite easily identifyable by a person.
```{r, echo=F, message=F, warning=F}
class_names <- c('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')


showDigit <- function(data_row, label) {
  image(
    t(apply(data_row, 2, rev)),
    col = gray.colors(255), xlab = label, ylab = ""
  )
}

showFirst <- function(data, label) {
  par(mfrow = c(3, 4))
  if (missing(label)) label <- rep("", 12)
  walk(1:12, ~showDigit(data[.,, ], label[.]))
}

showFirst(x_train, class_names[y_train+1])
```

These are the first 12 images from the test set, for which we are going to try to predict the categories.

```{r}
showFirst(x_test, )
```


## b) Train a fully connected deep network to predict items.

### Normalize the data similarly to what we saw with MNIST.
Before building the models we need some normalization. Currently we have values between 0 and 255 in our data. We want to normalize it and make sure all values are between 0 and 1, so we divide both the train and test set by 255. I also transform the y values to categorical.

```{r, message=F, warning=F}
train_x <- x_train / 255
test_x <- x_test / 255

train_y <- to_categorical(y_train, 10)
test_y <- to_categorical(y_test, 10)

```

```{r confusion matrix, include=F}
plotConfusionMatrix <- function(label, prediction) {
  bind_cols(label = label, predicted = prediction) %>%
    group_by(label, predicted) %>%
    summarize(N = n()) %>%
    ggplot(aes(label, predicted)) +
      geom_tile(aes(fill = N), colour = "white") +
      scale_x_continuous(breaks = 0:9) +
      scale_y_continuous(breaks = 0:9) +
      geom_text(aes(label = N), vjust = 1, color = "white") +
      scale_fill_viridis_c() +
      theme_bw() + theme(legend.position = "none")
}
```


### Experiment with network architectures and settings (number of hidden layers, number of nodes, activation functions, dropout, etc.)

### Model 1

I built a simple one hidden layer network with linear activation function in the hidden layer and 128 nodes in the layer. For the output layer I used the softmax activation function as we have a classification with more than 2 categories here.
The accuracy is around **84.66%** on the validation set. As even if we set the seed the neural net includes randomization, it is impossible to write the exact accuracy of the time you run it, so I can only say it is around a value. To solve this I commented out the fitting phase for all the models and saved out the results of my last run.  So if you run this code it only loads the saves models and you will get the same results as I got. I will only state the accuracy on the validation sets and also choose the best model based on this, as if we look at only the training set accuracy we can highly overfit that and than do not get a powerful model, and the test set will only be used to get the final accuracy of the models on new data. 



```{r, echo=F, message=F, warning=F}
model_1 <- keras_model_sequential()
model_1 %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 128, activation = 'linear') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_1)

compile(
  model_1,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# m1 <- fit(
#   model_1, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(m1, paste0(my_path, "Model1.rds"))
# save_model_hdf5(model_1, paste0(my_path, "Model_1.h5"))

m1 <- readRDS(paste0(my_path, "Model1.rds"))
model_1 <- load_model_hdf5(paste0(my_path, "Model_1.h5"))

#evaluate(model_1, test_x, test_y)
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(m1) +theme_light()
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
keras_predictions <- predict_classes(model_1, test_x)
plotConfusionMatrix(y_test, keras_predictions)
```
I tried to play around with the number of epochs as well as the batch size.
For me both setting the number of epochs lower and higher yielded worse accuracy, as well as changing the number of batches in either direction. So I decided to stuck with the 30 epochs and 128 batch size for all of my models! (*This was also helpful, as my computer needed 2 hours to run the models :), so keeping epochs smaller made its life a bit easier*) 

```{r epochs, include=F}
m1b <- fit(
  model_1, train_x, train_y,
  epochs = 15, batch_size = 128,
  validation_split = 0.2
)

# evaluate(model_1, test_x, test_y)
#### 84% accuracy on test set and 85.12% on validation

m1c <- fit(
  model_1, train_x, train_y,
  epochs = 45, batch_size = 128,
  validation_split = 0.2
)

# evaluate(model_1, test_x, test_y)
#### 82.87% on test set, 84.4% on validation
```

```{r, include=F}
plot(m1b) +theme_light()
```


```{r batch size, include=F}
m1d <- fit(
  model_1, train_x, train_y,
  epochs = 30, batch_size = 64,
  validation_split = 0.2
)

# evaluate(model_1, test_x, test_y)
#### 82.88% on test and 84.59% on validation
m1e <- fit(
  model_1, train_x, train_y,
  epochs = 30, batch_size = 256,
  validation_split = 0.2
)

# evaluate(model_1, test_x, test_y)
#### 83.65% on test and 84.68% on validation

### changing the batch size in either direction yields a worse accuracy
```

After I checked what happens when I change the batch size and the number of epochs I started playing around with the network architectures.


### Model 2

In the second model I changed the activation function to sigmoid, but kept all other things the same. We already see a nice improvement is the accuracy of this model as it reaches  **89.1%**.

```{r, echo=F, message=F, warning=F}
model_2 <- keras_model_sequential()
model_2 %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 128, activation = 'sigmoid') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_2)

compile(
  model_2,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# m2 <- fit(
#   model_2, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# 
# saveRDS(m2, paste0(my_path, "Model2.rds"))
# save_model_hdf5(model_2, paste0(my_path, "Model_2.h5"))

m2 <- readRDS(paste0(my_path, "Model2.rds"))
model_2 <- load_model_hdf5(paste0(my_path, "Model_2.h5"))

#evaluate(model_2, test_x, test_y)
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(m2) +theme_light()
```

```{r, include=F}
keras_predictions <- predict_classes(model_2, test_x)
plotConfusionMatrix(y_test, keras_predictions)
```

### Model 3

In model 3 I checked the relu activation function. It is advised to use this in hidden layers as the derivatives for sigmoid and TanH functions is bad and the linear function is not made for classification problems.  Actually, this case the anylsis prove this as this model yields most accuracy of **89.28%**.

```{r, echo=F, message=F, warning=F}
model_3 <- keras_model_sequential()
model_3 %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_3)

compile(
  model_3,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# m3 <- fit(
#   model_3, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(m3, paste0(my_path, "Model3.rds"))
# save_model_hdf5(model_3, paste0(my_path, "Model_3.h5"))

m3 <- readRDS(paste0(my_path, "Model3.rds"))
model_3 <- load_model_hdf5(paste0(my_path, "Model_3.h5"))

#evaluate(model_3, test_x, test_y)
```


```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(m3) +theme_light()
```

```{r, include=F}
keras_predictions <- predict_classes(model_3, test_x)
plotConfusionMatrix(y_test, keras_predictions)
```



### Model 4
After finding out the best activation function, let's increase the number of nodes in the hidden layer. This did increase the accuracy of the model in my case. It was **89.78%** on the validation. 

```{r, echo=F, message=F, warning=F}
model_4 <- keras_model_sequential()
model_4 %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 256, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_4)

compile(
  model_4,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# m4 <- fit(
#   model_4, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(m4, paste0(my_path, "Model4.rds"))
# save_model_hdf5(model_4, paste0(my_path, "Model_4.h5"))

m4 <- readRDS(paste0(my_path, "Model4.rds"))
model_4 <- load_model_hdf5(paste0(my_path, "Model_4.h5"))

#evaluate(model_4, test_x, test_y)
```


```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(m4) +theme_light()
```

```{r, include=F}
keras_predictions_valid <- predict_classes(model_4, test_x)
plotConfusionMatrix(y_test, keras_predictions_valid)
```


### Model 5

Let's also look at what happens if we add an other hidden layer to the model. In my case this does not improve accuracy by much, it was **89.3%** on the validation set. It is less than for model 4. 

```{r, echo=F, message=F, warning=F}
model_5 <- keras_model_sequential()
model_5 %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 256, activation = 'relu') %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_5)

compile(
  model_5,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# m5 <- fit(
#   model_5, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(m5, paste0(my_path, "Model5.rds"))
# save_model_hdf5(model_5, paste0(my_path, "Model_5.h5"))

m5 <- readRDS(paste0(my_path, "Model5.rds"))
model_5 <- load_model_hdf5(paste0(my_path, "Model_5.h5"))

#evaluate(model_5, test_x, test_y)
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(m5) +theme_light()
```

```{r, include=F}
keras_predictions_valid <- predict_classes(model_5, test_x)
plotConfusionMatrix(y_test, keras_predictions_valid)
```

## Model 6
In model 6 I included a drop out layer as well. Here I only have one hidden layer as adding another did not yield any improvement is accuracy and this layer has 256 nodes and relu as the activation function. This model yielded most accuracy, so including a droput layer made a better model. Accuracy was **89.57%**.

```{r, echo=F, message=F, warning=F}
model_6 <- keras_model_sequential()
model_6 %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 256, activation = 'relu') %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_6)

compile(
  model_6,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# m6 <- fit(
#   model_6, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(m6, paste0(my_path, "Model6.rds"))
# save_model_hdf5(model_6, paste0(my_path, "Model_6.h5"))

m6 <- readRDS(paste0(my_path, "Model6.rds"))
model_6 <- load_model_hdf5(paste0(my_path, "Model_6.h5"))

#evaluate(model_6, test_x, test_y)
```


```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(m6) +theme_light()
```

```{r, echo=F, message=F, warning=F, out.width="40%", fig.align='center'}
keras_predictions_valid <- predict_classes(model_6, test_x)
plotConfusionMatrix(y_test, keras_predictions_valid)
```



## c) Evaluate the model on the test set. How does test error compare to validation error?

I already evaluated all the models on the test set above. I have chosen the best model solely based on validation accuracy. It was model 4 in my case. It had one hidden layer with 256 neurons an no dropout layer and of course az output layer with 10 possible outcomes. The test accuracy for it was **88.73%**, compared to the validation accuracy of **89.78%**. For all the models sets errors and validation errors were pretty close to each other. In all the cases validation accuracy is slightly higher. This can be due to that validation data set was a bit larger with 12 000 images than the test set with 10 000 images. In all of the models the number of epochs were enough for the accuracy and loss function to flatten. 

```{r, echo=F}
evaluate(model_4, test_x, test_y)
```


## d) Try building a convolutional neural network and see if you can improve test set performance.

After building some basic networks, lets see if we can improve performance, by adding a convolutional layer

```{r, include=F}
train_x <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
test_x <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))
```

## CNN model 1

Add a convolutional layer to my net. This will have 32 filters, with a kernel size of 3x3. I also used a 2x2 max pooling after this. 
This model already performs better than the best simple net model with **90.28%** accuracy on the validation set. Proving already taht including a convolutional layer increases performance.

```{r, echo=F, message=F, warning=F}
cnn_1 <- keras_model_sequential()
cnn_1 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_1)
```

```{r cnn-setup, echo=F, message=F, warning=F, cache=T}
compile(
  cnn_1,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cm_1 <- fit(
#   cnn_1, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(cm_1, paste0(my_path, "CNN1.rds"))
# save_model_hdf5(cnn_1, paste0(my_path, "Cnn_1.h5"))

cm_1 <- readRDS(paste0(my_path, "CNN1.rds"))
cnn_1 <- load_model_hdf5(paste0(my_path, "Cnn_1.h5"))

#evaluate(cnn_1, test_x, test_y)
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(cm_1) +theme_light()
```

```{r, include=F}
keras_predictions_valid <- predict_classes(cnn_1, test_x)
plotConfusionMatrix(y_test, keras_predictions_valid)
```


## CNN model 2

In the next model I included an additional convolutional layer, but it has 64 3x3 filters. This increases accuracy to **90.46%**.

```{r, echo=F, message=F, warning=F}
cnn_2 <- keras_model_sequential()

cnn_2 %>% 
  layer_conv_2d(
    filters = 32, 
    kernel_size = c(3,3), 
    activation = 'relu',
    input_shape = c(28, 28, 1)) %>%
  layer_conv_2d(
    filters = 64, 
    kernel_size = c(3,3), 
    activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_2)
```

```{r, echo=F, message=F, warning=F, cache=T}
compile(
  cnn_2,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cm_2 <- fit(
#   cnn_2, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(cm_2, paste0(my_path, "CNN2.rds"))
# save_model_hdf5(cnn_2, paste0(my_path, "Cnn_2.h5"))

cm_2 <- readRDS(paste0(my_path, "CNN2.rds"))
cnn_2 <- load_model_hdf5(paste0(my_path, "Cnn_2.h5"))

#evaluate(cnn_2, test_x, test_y)
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(cm_2) +theme_light()
```


## CNN model 3
In the third convolutional net model I tried what happens if I increase the Kernel size to 5x5 in the first conv layer. This lead to a bit worse result with around **89.87%** accuracy in the validation set. 

```{r, echo=F, message=F, warning=F}
cnn_3 <- keras_model_sequential()

cnn_3 %>% 
  layer_conv_2d(
    filters = 32, 
    kernel_size = c(5,5), 
    activation = 'relu',
    input_shape = c(28, 28, 1)) %>%
  layer_conv_2d(
    filters = 64, 
    kernel_size = c(3,3), 
    activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_3)
```

```{r, echo=F, message=F, warning=F, cache=T}
compile(
  cnn_3,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cm_3 <- fit(
#   cnn_3, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(cm_3, paste0(my_path, "CNN3.rds"))
# save_model_hdf5(cnn_3, paste0(my_path, "Cnn_3.h5"))

cm_3 <- readRDS(paste0(my_path, "CNN3.rds"))
cnn_3 <- load_model_hdf5(paste0(my_path, "Cnn_3.h5"))

#evaluate(cnn_3, test_x, test_y)
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(cm_3) +theme_light()
```


## CNN model 4
I tried if I can improve further by including an additional convolutional layer with 128 filters. Surprisingly this did not improve my accuracy either. I got **86.98%**.

```{r, echo=F, message=F, warning=F}
cnn_4 <- keras_model_sequential()

cnn_4 %>% 
  layer_conv_2d(
    filters = 32, 
    kernel_size = c(3,3), 
    activation = 'relu',
    input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(
    filters = 64, 
    kernel_size = c(3,3), 
    activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(
    filters = 128, 
    kernel_size = c(3,3), 
    activation = 'relu',
    input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_4)
```

```{r, echo=F, message=F, warning=F, cache=T}
compile(
  cnn_4,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cm_4 <- fit(
#   cnn_4, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(cm_4, paste0(my_path, "CNN4.rds"))
# save_model_hdf5(cnn_4, paste0(my_path, "Cnn_4.h5"))

cm_4 <- readRDS(paste0(my_path, "CNN4.rds"))
cnn_4 <- load_model_hdf5(paste0(my_path, "Cnn_4.h5"))

#evaluate(cnn_4, test_x, test_y)
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(cm_4) +theme_light()
```



## CNN model 5
In the last convolutional network I added dropout layer after all the convolutional layers as a regularization technique. This also lead to lower accuracy than the 2nd model, here it is **88.06%**.

```{r, echo=F, message=F, warning=F}
cnn_5 <- keras_model_sequential()

cnn_5 %>% 
  layer_conv_2d(
    filters = 32, 
    kernel_size = c(3,3), 
    activation = 'relu',
    input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.15) %>%
  layer_conv_2d(
    filters = 64, 
    kernel_size = c(3,3), 
    activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_conv_2d(
    filters = 128, 
    kernel_size = c(3,3), 
    activation = 'relu',
    input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_5)
```

```{r, echo=F, cache=T}
compile(
  cnn_5,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cm_5 <- fit(
#   cnn_5, train_x, train_y,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.2
# )
# saveRDS(cm_5, paste0(my_path, "CNN5.rds"))
# save_model_hdf5(cnn_5, paste0(my_path, "Cnn_5.h5"))

cm_5 <- readRDS(paste0(my_path, "CNN5.rds"))
cnn_5 <- load_model_hdf5(paste0(my_path, "Cnn_5.h5"))

#evaluate(cnn_5, test_x, test_y)
```

```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}
plot(cm_5) +theme_light()
```


I have tried several convolutional models. I looked at models with different number of convolutional layers, different sized filters and different regularization with dpop out layers. For tme the best model was the second one, in which I had 2 convolutional layers both with 3x3 kernels, the first had 32 filters, the second had 64. I used max pooling and one drop out layer with a rate of 25%. With this model I reached **90.18%** accuracy on the test set compared to a **90.46%** on the validation set. And we also saw that adding convolutional layers to the neural nets improve their power.

```{r, echo=F}
evaluate(cnn_2, test_x, test_y)
```

Our models perform worse when predicting shirt, instead they usually predict pullover or T-shirt/top. This is reasonable as even a person could miscategorize these items as they can look really similar on these 28x28 pixel images.

*Note that computational limitations gave some barriers to build really deep models with high epoch number.*











